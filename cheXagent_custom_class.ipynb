{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Imports and definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from rich import print\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "\n",
    "\n",
    "class CheXpertDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        images,\n",
    "        finding_labels,\n",
    "        prompt_keys,\n",
    "        prompts_dict,\n",
    "        processor_pretrained_model=\"StanfordAIMI/CheXagent-8b\",\n",
    "    ):\n",
    "        self.images = images\n",
    "        self.finding_labels = finding_labels\n",
    "        self.prompt_keys = prompt_keys\n",
    "        self.prompts_dict = prompts_dict\n",
    "        self.processor = AutoProcessor.from_pretrained(\n",
    "            processor_pretrained_model, trust_remote_code=True\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.finding_labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        prompt_key = self.prompt_keys[index]\n",
    "        prompt = self.prompts_dict[prompt_key]\n",
    "        finding_label = self.finding_labels[index]\n",
    "        inputs = self.processor(\n",
    "            images=[image], text=f\" USER: <s>{prompt} ASSISTANT: <s>\", return_tensors=\"pt\"\n",
    "        )\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}  # Adjust as necessary\n",
    "\n",
    "        return inputs, image_path, prompt_key, finding_label\n",
    "\n",
    "\n",
    "def disp_df(dataframe):\n",
    "    display(dataframe.head())\n",
    "    print(f\"nRows: {dataframe.shape[0]:,}\\tnColumns: {dataframe.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input data for analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NIH Chest X-ray dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"image_index\", \"finding_labels\", \"follow_up_number\", \"patient_id\", \"patient_age\", \"patient_gender\", \"view_position\", \"original_image_width\", \"original_image_height\", \"original_image_pixel_spacing_x\", \"original_image_pixel_spacing_y\"]  # fmt: skip # nopep8\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"./data/NIH_Chest_X-ray_Dataset/Data_Entry_2017.csv\",\n",
    "    names=column_names,\n",
    "    header=0,\n",
    "    index_col=False,\n",
    ")\n",
    "\n",
    "disp_df(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IRF-relevant cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prompt dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_file_list = Path(\"data\").glob(\"prompts_ref*\")\n",
    "# prompt_file_list = Path(\"data\").glob(\"prompts.json\")\n",
    "prompts = {}\n",
    "for prompt_file in prompt_file_list:\n",
    "    with open(prompt_file.as_posix(), \"r\") as json_file:\n",
    "        prompt_i = json.load(json_file)\n",
    "        prompts.update(prompt_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NIH Chest X-ray dataset: define subset to analyze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (\n",
    "    \"Atelectasis\",  # relevant to IRF work\n",
    "    # \"Cardiomegaly\",\n",
    "    \"Consolidation\",  # relevant to IRF work\n",
    "    # \"Edema\",\n",
    "    \"Effusion\",  # relevant to IRF work\n",
    "    # \"Emphysema\",\n",
    "    # \"Fibrosis\",\n",
    "    # \"Hernia\",\n",
    "    \"Infiltration\",  # relevant to IRF work\n",
    "    # \"Mass\",\n",
    "    # \"Nodule\",\n",
    "    # \"Pleural_Thickening\",\n",
    "    \"Pneumonia\",  # relevant to IRF work\n",
    "    # \"Pneumothorax\",\n",
    "    \"No Finding\",  # relevant to IRF work\n",
    ")\n",
    "labels = \"|\".join(labels)\n",
    "\n",
    "labels_ignore = (\n",
    "    # \"Atelectasis\", # relevant to IRF work\n",
    "    \"Cardiomegaly\",\n",
    "    # \"Consolidation\", # relevant to IRF work\n",
    "    \"Edema\",\n",
    "    # \"Effusion\", # relevant to IRF work\n",
    "    \"Emphysema\",\n",
    "    \"Fibrosis\",\n",
    "    \"Hernia\",\n",
    "    # \"Infiltration\", # relevant to IRF work\n",
    "    \"Mass\",\n",
    "    \"Nodule\",\n",
    "    \"Pleural_Thickening\",\n",
    "    # \"Pneumonia\", # relevant to IRF work\n",
    "    \"Pneumothorax\",\n",
    "    # \"No Finding\", # relevant to IRF work\n",
    ")\n",
    "labels_ignore = \"|\".join(labels_ignore)\n",
    "\n",
    "nih_cases_to_analyze = data.loc[\n",
    "    data[\"finding_labels\"].str.contains(labels)\n",
    "    & ~data[\"finding_labels\"].str.contains(labels_ignore),\n",
    "    \"image_index\",\n",
    "].to_numpy()\n",
    "\n",
    "pairs = list(product(nih_cases_to_analyze, prompts.keys()))\n",
    "df_inputs = pd.DataFrame(pairs, columns=[\"image_index\", \"prompt_key\"])\n",
    "df_inputs[\"finding_labels\"] = df_inputs[\"image_index\"].map(\n",
    "    data.set_index(\"image_index\")[\"finding_labels\"].to_dict()\n",
    ")\n",
    "\n",
    "disp_df(df_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load analyzed image/prompt combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = [x.as_posix() for x in Path(\"output\").rglob(\"disease_classification_QA*.csv\")]\n",
    "# filelist = [x.as_posix() for x in Path(\"output\").rglob(\"CheXagent_results_on_NIH_CXR.csv.gz\")]\n",
    "\n",
    "if filelist == []:\n",
    "    df_results_prev = pd.DataFrame(\n",
    "        columns=[\"image_index\", \"finding_labels\", \"prompt_key\", \"response\"]\n",
    "    )\n",
    "else:\n",
    "    df_results_prev = pd.DataFrame()\n",
    "    for f in filelist:\n",
    "        df_i = pd.read_csv(\n",
    "            f, usecols=[\"image_index\", \"finding_labels\", \"prompt_key\", \"response\"], dtype=str\n",
    "        )\n",
    "        df_results_prev = pd.concat([df_results_prev, df_i])\n",
    "\n",
    "    df_results_prev = df_results_prev.drop_duplicates(ignore_index=True)\n",
    "\n",
    "disp_df(df_results_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Input image/prompt combinations that have not been analyzed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_analyzed = pd.merge(left=df_inputs, right=df_results_prev, how=\"left\").pipe(\n",
    "    lambda x: x.loc[x[\"response\"].isna(), [\"image_index\", \"finding_labels\", \"prompt_key\"]]\n",
    ")\n",
    "\n",
    "disp_df(df_not_analyzed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### sort the input dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show least analyzed finding_label first\n",
    "count_map = df_results_prev[\"finding_labels\"].value_counts().to_dict()\n",
    "df_not_analyzed[\"label_count\"] = df_not_analyzed[\"finding_labels\"].map(count_map)\n",
    "\n",
    "# create int dtype of prompt key column\n",
    "df_not_analyzed[\"prompt_key_int\"] = df_not_analyzed[\"prompt_key\"].astype(int)\n",
    "\n",
    "df_not_analyzed = df_not_analyzed.sort_values(\n",
    "    [\"label_count\", \"image_index\", \"prompt_key_int\"], ignore_index=True\n",
    ").drop(columns=[\"label_count\", \"prompt_key_int\"])\n",
    "disp_df(df_not_analyzed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### add image filepaths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import filepaths\n",
    "df_full_paths = (\n",
    "    pd.read_csv(\"data/NIH_Chest_X-ray_image_filepaths.csv\")\n",
    "    .set_index(\"image_index\")[\"image_path\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "df_not_analyzed[\"image_path\"] = df_not_analyzed[\"image_index\"].map(df_full_paths)\n",
    "\n",
    "disp_df(df_not_analyzed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### delete unused objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (\n",
    "    column_names,\n",
    "    count_map,\n",
    "    data,\n",
    "    df_full_paths,\n",
    "    df_i,\n",
    "    df_inputs,\n",
    "    df_results_prev,\n",
    "    f,\n",
    "    filelist,\n",
    "    json_file,\n",
    "    labels,\n",
    "    labels_ignore,\n",
    "    nih_cases_to_analyze,\n",
    "    pairs,\n",
    "    prompt_file,\n",
    "    prompt_file_list,\n",
    "    prompt_i,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### define filepath for results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_parent = Path(\"output/NIH_Chest_Xray_IRFrelevant_findings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CheXagent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### initialize `CheXpertDataset`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CheXpertDataset(\n",
    "    df_not_analyzed[\"image_path\"].values,\n",
    "    df_not_analyzed[\"finding_labels\"].values,\n",
    "    df_not_analyzed[\"prompt_key\"].values,\n",
    "    prompts,\n",
    ")\n",
    "# keys=[key for key in prompts.keys()]\n",
    "# dataset = CheXpertDataset(\n",
    "#     [\"data/DRR.jpg\"] * len(keys),\n",
    "#     [\"No Finding\"] * len(keys),\n",
    "#     [key for key in prompts.keys()],\n",
    "#     prompts,\n",
    "# )\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)  # Batch size set to 1 for simplicity\n",
    "processor = dataset.processor\n",
    "\n",
    "# Load the model and set it to evaluation mode\n",
    "device = \"cuda\"\n",
    "dtype = torch.float16\n",
    "\n",
    "model_name = \"StanfordAIMI/CheXagent-8b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=dtype, trust_remote_code=True\n",
    ").to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load generation config if needed\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### run the analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "datetime_str = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_file_path = csv_file_parent.joinpath(f\"disease_classification_QA_{datetime_str}.csv\").as_posix()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "with open(csv_file_path, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"image_index\", \"finding_labels\", \"prompt_key\", \"response\"])\n",
    "\n",
    "    for batch in tqdm(data_loader, total=len(data_loader), desc=\"Processing images\"):\n",
    "        inputs = batch[0]\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "        image_path, prompt_key, finding_label = [x[0] for x in batch[1:]]\n",
    "        image_index = Path(image_path).name\n",
    "\n",
    "        # Generate text; adjust depending on your model's API\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            generation_config=generation_config,\n",
    "            pad_token_id=processor.tokenizer.eos_token_id,\n",
    "        )[0]\n",
    "\n",
    "        # Decode and print the generated text\n",
    "        generated_text = processor.tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "        row_data = [image_index, finding_label, prompt_key, generated_text]\n",
    "        writer.writerow(row_data)\n",
    "\n",
    "warnings.resetwarnings()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (chexagent)",
   "language": "python",
   "name": "chexagent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
