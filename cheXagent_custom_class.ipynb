{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Imports and definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from rich import print\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "\n",
    "\n",
    "class CheXpertDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, images, finding_labels, prompt_keys, prompts_dict, processor_pretrained_model=\"StanfordAIMI/CheXagent-8b\"\n",
    "    ):\n",
    "        self.images = images\n",
    "        self.finding_labels = finding_labels\n",
    "        self.prompt_keys = prompt_keys\n",
    "        self.prompts_dict = prompts_dict\n",
    "        self.processor = AutoProcessor.from_pretrained(processor_pretrained_model, trust_remote_code=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.finding_labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        prompt_key = self.prompt_keys[index]\n",
    "        prompt = self.prompts_dict[prompt_key]\n",
    "        finding_label = self.finding_labels[index]\n",
    "        inputs = self.processor(images=[image], text=f\" USER: <s>{prompt} ASSISTANT: <s>\", return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}  # Adjust as necessary\n",
    "\n",
    "        return inputs, image_path, prompt_key, finding_label\n",
    "\n",
    "\n",
    "def disp_df(dataframe):\n",
    "    display(dataframe.head())\n",
    "    print(f\"nRows: {dataframe.shape[0]:,}\\tnColumns: {dataframe.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input data for analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NIH Chest X-ray dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"image_index\", \"finding_labels\", \"follow_up_number\", \"patient_id\", \"patient_age\", \"patient_gender\", \"view_position\", \"original_image_width\", \"original_image_height\", \"original_image_pixel_spacing_x\", \"original_image_pixel_spacing_y\"]  # fmt: skip # nopep8\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"./data/NIH_Chest_X-ray_Dataset/Data_Entry_2017.csv\",\n",
    "    names=column_names,\n",
    "    header=0,\n",
    "    index_col=False,\n",
    ")\n",
    "\n",
    "disp_df(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prompt dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import prompts dictionary\n",
    "with open(\"output/prompts.json\", \"r\") as json_file:\n",
    "    prompts = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NIH Chest X-ray dataset: single finding cases with prompt_key column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_single_finding = data.loc[\n",
    "    (data[\"finding_labels\"].str.count(\"\\\\|\") + 1) == 1, [\"image_index\", \"finding_labels\"]\n",
    "].copy()\n",
    "data_single_finding_prompts = pd.DataFrame()\n",
    "for prompt_key in prompts.keys():\n",
    "    df_i = data_single_finding.copy()\n",
    "    df_i[\"prompt_key\"] = prompt_key\n",
    "    data_single_finding_prompts = pd.concat([data_single_finding_prompts, df_i])\n",
    "\n",
    "disp_df(data_single_finding_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load analyzed image/prompt combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = [x.as_posix() for x in Path(\"output\").glob(\"disease_classification_QA*.csv\")]\n",
    "\n",
    "df_results_prev = pd.DataFrame()\n",
    "for f in filelist:\n",
    "    df_i = pd.read_csv(f, usecols=[\"image_index\", \"finding_labels\", \"prompt_key\", \"response\"], dtype=str)\n",
    "    df_results_prev = pd.concat([df_results_prev, df_i])\n",
    "\n",
    "df_results_prev = df_results_prev.drop_duplicates(ignore_index=True)\n",
    "disp_df(df_results_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Input image/prompt combinations that have not been analyzed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_not_analyzed = pd.merge(left=data_single_finding_prompts, right=df_results_prev, how=\"left\").pipe(\n",
    "    lambda x: x.loc[x[\"response\"].isna(), [\"image_index\", \"finding_labels\", \"prompt_key\"]]\n",
    ")\n",
    "\n",
    "disp_df(df_not_analyzed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### sort the input dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show least analyzed finding_label first\n",
    "count_map = df_results_prev[\"finding_labels\"].value_counts().to_dict()\n",
    "df_not_analyzed[\"label_count\"] = df_not_analyzed[\"finding_labels\"].map(count_map)\n",
    "\n",
    "# create int dtype of prompt key column\n",
    "df_not_analyzed[\"prompt_key_int\"] = df_not_analyzed[\"prompt_key\"].astype(int)\n",
    "\n",
    "df_not_analyzed = df_not_analyzed.sort_values([\"label_count\", \"image_index\", \"prompt_key_int\"], ignore_index=True).drop(\n",
    "    columns=[\"label_count\", \"prompt_key_int\"]\n",
    ")\n",
    "disp_df(df_not_analyzed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### add image filepaths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import filepaths\n",
    "df_full_paths = (\n",
    "    pd.read_csv(\"./output/single_finding_datalist.csv\", usecols=[\"image_index\", \"image_path\"])\n",
    "    .set_index(\"image_index\")[\"image_path\"]\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "df_not_analyzed[\"image_path\"] = df_not_analyzed[\"image_index\"].map(df_full_paths)\n",
    "\n",
    "disp_df(df_not_analyzed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CheXagent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### delete unused objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (\n",
    "    column_names,\n",
    "    count_map,\n",
    "    data,\n",
    "    data_single_finding,\n",
    "    data_single_finding_prompts,\n",
    "    df_full_paths,\n",
    "    df_i,\n",
    "    df_results_prev,\n",
    "    f,\n",
    "    filelist,\n",
    "    json_file,\n",
    "    prompt_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### initialize `CheXpertDataset`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CheXpertDataset(\n",
    "    df_not_analyzed[\"image_path\"].values,\n",
    "    df_not_analyzed[\"finding_labels\"].values,\n",
    "    df_not_analyzed[\"prompt_key\"].values,\n",
    "    prompts,\n",
    ")\n",
    "# keys=[key for key in prompts.keys()]\n",
    "# dataset = CheXpertDataset(\n",
    "#     [\"data/DRR.jpg\"] * len(keys),\n",
    "#     [\"No Finding\"] * len(keys),\n",
    "#     [key for key in prompts.keys()],\n",
    "#     prompts,\n",
    "# )\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)  # Batch size set to 1 for simplicity\n",
    "processor = dataset.processor\n",
    "\n",
    "# Load the model and set it to evaluation mode\n",
    "device = \"cuda\"\n",
    "dtype = torch.float16\n",
    "\n",
    "model_name = \"StanfordAIMI/CheXagent-8b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=dtype, trust_remote_code=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load generation config if needed\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### run the analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "datetime_str = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_file_path = f\"output/disease_classification_QA_{datetime_str}.csv\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "with open(csv_file_path, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"image_index\", \"finding_labels\", \"prompt_key\", \"response\"])\n",
    "\n",
    "    for batch in tqdm(data_loader, total=len(data_loader), desc=\"Processing images\"):\n",
    "        inputs = batch[0]\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "        image_path, prompt_key, finding_label = [x[0] for x in batch[1:]]\n",
    "        image_index = Path(image_path).name\n",
    "\n",
    "        # Generate text; adjust depending on your model's API\n",
    "        output = model.generate(\n",
    "            **inputs, generation_config=generation_config, pad_token_id=processor.tokenizer.eos_token_id\n",
    "        )[0]\n",
    "\n",
    "        # Decode and print the generated text\n",
    "        generated_text = processor.tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "        row_data = [image_index, finding_label, prompt_key, generated_text]\n",
    "        writer.writerow(row_data)\n",
    "\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.read_csv(\"output/DRR_cheXagent.csv\")\n",
    "display(out)\n",
    "[print(x) for x in out[\"response\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "total_seg_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
