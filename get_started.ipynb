{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CheXagent disease classification on [NIH Chest X-ray data](https://www.kaggle.com/datasets/nih-chest-xrays/sample/data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from rich import print\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    hamming_loss,\n",
    "    jaccard_score,\n",
    "    multilabel_confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import SVC\n",
    "from skmultilearn.dataset import load_dataset\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "\n",
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\", font_scale=1.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Function definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url):\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return Image.open(io.BytesIO(resp.content)).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def generate(images, prompt, processor, model, device, dtype, generation_config):\n",
    "    inputs = processor(images=images[:2], text=f\" USER: <s>{prompt} ASSISTANT: <s>\", return_tensors=\"pt\").to(\n",
    "        device=device, dtype=dtype\n",
    "    )\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        generation_config=generation_config,\n",
    "        # this silences \"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.\"\n",
    "        pad_token_id=processor.tokenizer.eos_token_id,\n",
    "    )[0]\n",
    "    response = processor.tokenizer.decode(output, skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "\n",
    "def plot_img(image, title):\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_ypred(response, labels):\n",
    "    return tuple(label for label in labels if label in response)\n",
    "\n",
    "\n",
    "def get_tps(series):\n",
    "    tps = [x for x in series[\"y_pred\"] if x in series[\"y_true\"]]\n",
    "    return tuple(tps)\n",
    "\n",
    "\n",
    "def get_fps(series):\n",
    "    fps = [x for x in series[\"y_pred\"] if x not in series[\"y_true\"]]\n",
    "    return tuple(fps)\n",
    "\n",
    "\n",
    "def get_tns(series, labels):\n",
    "    tns = [x for x in labels if (x not in series[\"y_true\"]) and (x not in series[\"y_pred\"])]\n",
    "    return tuple(tns)\n",
    "\n",
    "\n",
    "def get_fns(series):\n",
    "    fns = [x for x in series[\"y_true\"] if x not in series[\"y_pred\"]]\n",
    "    return tuple(fns)\n",
    "\n",
    "\n",
    "def disp_df(dataframe):\n",
    "    display(dataframe.head())\n",
    "    print(f\"nRows: {dataframe.shape[0]:,}\\tnColumns: {dataframe.shape[1]}\")\n",
    "\n",
    "\n",
    "def compute_mlc_metrics_wide(df, labels):\n",
    "    y_true = df[\"y_true\"]\n",
    "    y_pred = df[\"y_pred\"]\n",
    "    mlb = MultiLabelBinarizer(classes=labels)\n",
    "    ytrue = mlb.fit_transform(y_true)\n",
    "    ypred = mlb.fit_transform(y_pred)\n",
    "\n",
    "    # Multi-label averaging parameter options: \"macro\", \"weighted\", \"micro\", \"samples\", and None\n",
    "    avg = None\n",
    "\n",
    "    index_label_zero_div_map = {0.0: \"0\", 1.0: \"1\", \"warn\": \"warn\", np.nan: \"nan\"}\n",
    "\n",
    "    metrics_wide = pd.DataFrame()\n",
    "    for zero_div in [0.0, 1.0, np.nan]:\n",
    "\n",
    "        # zero-div param for jaccard score cannot be NAN\n",
    "        if isinstance(zero_div, float) and np.isnan(zero_div):\n",
    "            zero_div_j = 0.0\n",
    "        else:\n",
    "            zero_div_j = zero_div\n",
    "        index_label_zero_div = index_label_zero_div_map[zero_div]\n",
    "\n",
    "        jaccard = jaccard_score(ytrue, ypred, average=avg, zero_division=zero_div_j)\n",
    "        precision = precision_score(ytrue, ypred, average=avg, zero_division=zero_div)\n",
    "        recall = recall_score(ytrue, ypred, average=avg, zero_division=zero_div)\n",
    "        f1 = f1_score(ytrue, ypred, average=avg, zero_division=zero_div)\n",
    "\n",
    "        df_i = pd.DataFrame(\n",
    "            np.vstack([jaccard, precision, recall, f1]),\n",
    "            columns=mlb.classes_.tolist(),\n",
    "            index=[\"jaccard\", \"precision\", \"recall\", \"f1\"],\n",
    "        )\n",
    "        df_i.index.name = \"metric\"\n",
    "        df_i[\"zerodiv_opt\"] = index_label_zero_div\n",
    "        df_i = df_i.reset_index()\n",
    "        df_i = df_i[[\"zerodiv_opt\", \"metric\"] + mlb.classes_.tolist()]\n",
    "\n",
    "        metrics_wide = pd.concat([metrics_wide, df_i], ignore_index=True)\n",
    "\n",
    "    metrics_wide = metrics_wide.sort_values(\"metric\").reset_index(drop=True)\n",
    "\n",
    "    return metrics_wide\n",
    "\n",
    "\n",
    "def compute_mlc_metrics(df, labels):\n",
    "    y_true = df[\"y_true\"]\n",
    "    y_pred = df[\"y_pred\"]\n",
    "    mlb = MultiLabelBinarizer(classes=labels)\n",
    "    ytrue = mlb.fit_transform(y_true)\n",
    "    ypred = mlb.fit_transform(y_pred)\n",
    "\n",
    "    index_label_zero_div_map = {0.0: \"0\", 1.0: \"1\", \"warn\": \"warn\", np.nan: \"nan\"}\n",
    "\n",
    "    metrics = pd.DataFrame(\n",
    "        columns=[\"avg_opt\", \"zerodiv_opt\", \"accuracy\", \"hammingloss\", \"jaccard\", \"precision\", \"recall\", \"f1\"]\n",
    "    )\n",
    "\n",
    "    idx = 0\n",
    "    for avg in [\"macro\", \"weighted\", \"micro\", \"samples\"]:\n",
    "        for zero_div in [0.0, 1.0, np.nan]:\n",
    "            # set avg param index label\n",
    "            if avg:\n",
    "                index_label_avg = avg\n",
    "            else:\n",
    "                index_label_avg = \"none\"\n",
    "\n",
    "            # zero-div param for jaccard score cannot be NAN\n",
    "            if isinstance(zero_div, float) and np.isnan(zero_div):\n",
    "                zero_div_j = 0.0\n",
    "            else:\n",
    "                zero_div_j = zero_div\n",
    "            index_label_zero_div = index_label_zero_div_map[zero_div]\n",
    "\n",
    "            accuracy = accuracy_score(ytrue, ypred)\n",
    "            hammingloss = hamming_loss(ytrue, ypred)\n",
    "            jaccard = jaccard_score(ytrue, ypred, average=avg, zero_division=zero_div_j)\n",
    "            precision = precision_score(ytrue, ypred, average=avg, zero_division=zero_div)\n",
    "            recall = recall_score(ytrue, ypred, average=avg, zero_division=zero_div)\n",
    "            f1 = f1_score(ytrue, ypred, average=avg, zero_division=zero_div)\n",
    "\n",
    "            metrics.loc[idx, :] = (\n",
    "                index_label_avg,\n",
    "                index_label_zero_div,\n",
    "                accuracy,\n",
    "                hammingloss,\n",
    "                jaccard,\n",
    "                precision,\n",
    "                recall,\n",
    "                f1,\n",
    "            )\n",
    "            idx += 1\n",
    "    metrics[[\"accuracy\", \"hammingloss\", \"jaccard\", \"precision\", \"recall\", \"f1\"]] = metrics.loc[\n",
    "        :, [\"accuracy\", \"hammingloss\", \"jaccard\", \"precision\", \"recall\", \"f1\"]\n",
    "    ].astype(float)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NIH Chest X-ray dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"image_index\", \"finding_labels\", \"follow_up_number\", \"patient_id\", \"patient_age\", \"patient_gender\", \"view_position\", \"original_image_width\", \"original_image_height\", \"original_image_pixel_spacing_x\", \"original_image_pixel_spacing_y\"]  # fmt: skip # nopep8\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"./data/NIH_Chest_X-ray_Dataset/Data_Entry_2017.csv\",\n",
    "    names=column_names,\n",
    "    header=0,\n",
    "    index_col=False,\n",
    ")\n",
    "display(data.head())\n",
    "print(f\"Rows: {data.shape[0]:,}\\tColumns: {data.shape[1]}\")\n",
    "print(f\"Number of patients: {data['patient_id'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Visualize data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLDEN = (1 + 5**0.5) / 2\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.countplot(x=data[\"patient_gender\"])\n",
    "plt.gca().set(title=\"Distribution of Patient Gender\", xlabel=None, ylabel=\"Counts\")\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "sns.histplot(data=data[data[\"patient_age\"] < 130], x=\"patient_age\", bins=20, kde=True)\n",
    "plt.gca().set(title=\"Distribution of Patient Age\", xlabel=\"Age\", ylabel=\"Counts\")\n",
    "\n",
    "# Create boolean columns for each pathology type\n",
    "pathology_list = [\"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\", \"Effusion\", \"Emphysema\", \"Fibrosis\", \"Hernia\", \"Infiltration\", \"Mass\", \"Nodule\", \"Pleural_Thickening\", \"Pneumonia\", \"Pneumothorax\"]  # fmt: skip # nopep8\n",
    "for pathology in pathology_list:\n",
    "    data[pathology] = data[\"finding_labels\"].apply(lambda x: 1 if pathology in x else 0)\n",
    "\n",
    "data[\"No_Findings\"] = data[\"finding_labels\"].apply(lambda x: 1 if \"No Finding\" in x else 0)\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.barplot(data.iloc[:, 11:-1].sum(axis=1).value_counts(), orient=\"h\")\n",
    "plt.gca().set(title=\"Number of findings per image\", xlabel=\"Counts\", ylabel=\"Number of findings\")\n",
    "\n",
    "# Sum of values across selected columns and reset index for Seaborn\n",
    "sum_data = data.iloc[:, 11:].sum().reset_index()\n",
    "sum_data.columns = [\"Feature\", \"Total\"]\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "sns.barplot(x=\"Total\", y=\"Feature\", data=sum_data)\n",
    "plt.gca().set(title=\"Disease Classes\", xlabel=\"Counts\", ylabel=None)\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "sns.barplot(x=\"Total\", y=\"Feature\", data=sum_data.loc[sum_data[\"Feature\"] != \"No_Findings\", :])\n",
    "plt.gca().set(title=\"Disease Classes (exluding no disease class)\", xlabel=\"Counts\", ylabel=None)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### remove calculated columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[column_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze prompts and responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### import results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filelist = [x.as_posix() for x in Path(\"output\").rglob(\"disease_classification_QA*.csv\")]\n",
    "filelist = [x.as_posix() for x in Path(\"output\").rglob(\"CheXagent_results_on_NIH_CXR.csv.gz\")]\n",
    "results = pd.DataFrame()\n",
    "for f in filelist:\n",
    "    df_i = pd.read_csv(f, usecols=[\"image_index\", \"finding_labels\", \"prompt_key\", \"response\"], dtype=str)\n",
    "    results = pd.concat([results, df_i])\n",
    "\n",
    "results = results.drop_duplicates(ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (\n",
    "    \"Atelectasis\",\n",
    "    \"Cardiomegaly\",\n",
    "    \"Consolidation\",\n",
    "    \"Edema\",\n",
    "    \"Effusion\",\n",
    "    \"Emphysema\",\n",
    "    \"Fibrosis\",\n",
    "    \"Hernia\",\n",
    "    \"Infiltration\",\n",
    "    \"Mass\",\n",
    "    \"Nodule\",\n",
    "    \"Pleural_Thickening\",\n",
    "    \"Pneumonia\",\n",
    "    \"Pneumothorax\",\n",
    "    # \"No Finding\",\n",
    ")\n",
    "# results[\"y_true\"] = results[\"finding_labels\"].str.split(\"|\")\n",
    "results[\"y_true\"] = results[\"finding_labels\"].apply(lambda x: tuple(x.replace(\"No Finding\", \"\").split(\"|\")))\n",
    "results[\"y_pred\"] = results[\"response\"].apply(lambda x: get_ypred(x, labels))\n",
    "results[\"n_labels\"] = results[\"y_true\"].apply(len)\n",
    "results[\"response_num_findings\"] = results[\"y_pred\"].apply(len)\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=labels)\n",
    "ytrue = mlb.fit_transform(results[\"y_true\"])\n",
    "ypred = mlb.fit_transform(results[\"y_pred\"])\n",
    "\n",
    "results[\"true_positives\"] = results.apply(get_tps, axis=1)\n",
    "results[\"false_positives\"] = results.apply(get_fps, axis=1)\n",
    "results[\"true_negatives\"] = results.apply(lambda x: get_tns(x, labels), axis=1)\n",
    "results[\"false_negatives\"] = results.apply(get_fns, axis=1)\n",
    "\n",
    "disp_df(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make \"prompt_key\" categorical\n",
    "prompt_file_list = Path(\"data\").glob(\"prompts*\")\n",
    "prompts = {}\n",
    "for prompt_file in prompt_file_list:\n",
    "    with open(prompt_file.as_posix(), \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "        prompts.update(data)\n",
    "\n",
    "prompt_key_order = [int(key) for key in prompts.keys()]\n",
    "prompt_key_order.sort()\n",
    "prompt_key_order = [str(key) for key in prompt_key_order]\n",
    "\n",
    "results[\"prompt_key\"] = pd.Categorical(results[\"prompt_key\"], ordered=True, categories=prompt_key_order)\n",
    "\n",
    "# make response_num_findings, and make categorical\n",
    "# response_num_findings_order = results[\"response_num_findings\"].drop_duplicates().sort_values().values\n",
    "# results[\"response_num_findings\"] = pd.Categorical(\n",
    "#     results[\"response_num_findings\"], ordered=True, categories=response_num_findings_order\n",
    "# )\n",
    "\n",
    "display(results.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### analyze response strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[\"true_positives_len\"] = results[\"true_positives\"].transform(len)\n",
    "# results[\"false_positives_len\"] = results[\"false_positives\"].transform(len)\n",
    "# results[\"true_negatives_len\"] = results[\"true_negatives\"].transform(len)\n",
    "# results[\"false_negatives_len\"] = results[\"false_negatives\"].transform(len)\n",
    "\n",
    "# results[\"accuracy\"] = (results[\"true_positives_len\"] + results[\"true_negatives_len\"]) / 15\n",
    "# results[\"recall\"] = results[\"true_positives_len\"] / (results[\"true_positives_len\"] + results[\"false_negatives_len\"])\n",
    "# results[\"precision\"] = results[\"true_positives_len\"] / (results[\"true_positives_len\"] + results[\"false_positives_len\"])\n",
    "# results[\"f1\"] = (2 * ((results[\"precision\"] * results[\"recall\"]) / (results[\"precision\"] + results[\"recall\"]))).fillna(\n",
    "#     0.0\n",
    "# )\n",
    "# results[\"specificity\"] = results[\"true_negatives_len\"] / (\n",
    "#     results[\"true_negatives_len\"] + results[\"false_positives_len\"]\n",
    "# )\n",
    "\n",
    "# disp_df(results)\n",
    "# remove outlier case\n",
    "# results = results.loc[results[\"response_num_findings\"] != 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate MLC task of the different prompts\n",
    "\n",
    "[Comprehensive comparative study of multi-label classification methods](https://doi.org/10.1016/j.eswa.2022.117215)\n",
    "\n",
    "4.5 Statistical evaluation if performance of the methods are statisitically significant:\n",
    "\n",
    "- corrected Friedman test (non-parametric multiple hypothesis test)\n",
    "- post-hoc Nemenyi test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Findings: Three or more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Determine `scikit-learn` metric params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prompts used three or more findings\n",
    "with open(\"data/prompts_three_or_more_findings.json\", \"r\") as json_file:\n",
    "    prompts = json.load(json_file)\n",
    "\n",
    "# subset results\n",
    "df_res = results[results[\"prompt_key\"] == \"1\"].copy()\n",
    "\n",
    "disp_df(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate one particular label and analyze one binary case\n",
    "true_0, pred_0 = ytrue[df_res.index][:, 0], ypred[df_res.index][:, 0]\n",
    "temp = pd.DataFrame({\"true\": true_0, \"pred\": pred_0})\n",
    "\n",
    "temp[\"true_pos\"] = 0.0\n",
    "temp.loc[(temp[\"true\"] == 1) & (temp[\"pred\"] == 1), \"true_pos\"] = 1\n",
    "\n",
    "temp[\"true_neg\"] = 0.0\n",
    "temp.loc[(temp[\"true\"] == 0) & (temp[\"pred\"] == 0), \"true_neg\"] = 1\n",
    "\n",
    "temp[\"false_pos\"] = 0.0\n",
    "temp.loc[(temp[\"true\"] == 0) & (temp[\"pred\"] == 1), \"false_pos\"] = 1\n",
    "\n",
    "temp[\"false_neg\"] = 0.0\n",
    "temp.loc[(temp[\"true\"] == 1) & (temp[\"pred\"] == 0), \"false_neg\"] = 1\n",
    "\n",
    "disp_df(temp)\n",
    "\n",
    "val_binary = recall_score(true_0, pred_0, average=\"binary\", zero_division=0.0)\n",
    "val_micro = recall_score(true_0, pred_0, average=\"micro\", zero_division=0.0)\n",
    "val_macro = recall_score(true_0, pred_0, average=\"macro\", zero_division=0.0)\n",
    "print(f\"recall(avg='binary'): {val_binary}\")\n",
    "print(f\"recall(avg='micro' or 'weighted'): {val_micro}\")\n",
    "print(f\"recall(avg='macro'): {val_macro}\")\n",
    "\n",
    "# this matches average = \"binary\"\n",
    "val = temp[\"true_pos\"].sum() / (temp[\"true_pos\"].sum() + temp[\"false_neg\"].sum())\n",
    "print(f\"replicate metric for avg='binary': {val}\")\n",
    "\n",
    "# this matches average = \"micro\" or \"weighted\"\n",
    "val = (temp[\"true_pos\"].sum() + temp[\"true_neg\"].sum()) / temp.shape[0]\n",
    "print(f\"replicate metric for avg='micro' or 'weighted': {val}\")\n",
    "\n",
    "# this matches average = \"macro\"\n",
    "val = recall_score(true_0, pred_0, average=None, zero_division=0.0).mean()\n",
    "print(f\"replicate metric for avg='macro': {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_wide = compute_mlc_metrics_wide(df_res, labels)\n",
    "metrics = compute_mlc_metrics(df_res, labels)\n",
    "display(metrics_wide)\n",
    "display(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_wide.drop_duplicates(subset=metrics_wide.select_dtypes(include=\"number\").columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.drop_duplicates(subset=metrics.select_dtypes(include=\"number\").columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.set_index([\"avg_opt\", \"zerodiv_opt\"]).iloc[:, -4:].rank(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.set_index([\"avg_opt\", \"zerodiv_opt\"]).iloc[:, -4:].rank(ascending=False).mean(axis=1).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_keys = results[\"prompt_key\"].unique()\n",
    "\n",
    "df_agg_wide = pd.DataFrame()\n",
    "df_agg = pd.DataFrame()\n",
    "for prompt_key in prompt_keys:\n",
    "    df_input = results[results[\"prompt_key\"] == prompt_key].copy()\n",
    "\n",
    "    metrics_wide_i = compute_mlc_metrics_wide(df_input, labels)\n",
    "    metrics_wide_i[\"prompt_key\"] = prompt_key\n",
    "    df_agg_wide = pd.concat([df_agg_wide, metrics_wide_i], ignore_index=True)\n",
    "\n",
    "    metrics_i = compute_mlc_metrics(df_input, labels)\n",
    "    metrics_i[\"prompt_key\"] = prompt_key\n",
    "    df_agg = pd.concat([df_agg, metrics_i], ignore_index=True)\n",
    "\n",
    "display(df_agg_wide)\n",
    "display(df_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\", font_scale=1.15)\n",
    "\n",
    "plotdf = (\n",
    "    df_agg.loc[df_agg[\"zerodiv_opt\"] == \"nan\"]\n",
    "    .loc[df_agg[\"avg_opt\"] == \"samples\"]\n",
    "    # .loc[df_agg[\"prompt_key\"].astype(int).isin(np.arange(1, 11))]\n",
    "    .drop(columns=[\"avg_opt\", \"zerodiv_opt\"])\n",
    "    .melt(id_vars=[\"prompt_key\"], var_name=\"metric\")\n",
    ")\n",
    "plotdf[\"prompt_key\"] = pd.Categorical(plotdf[\"prompt_key\"].astype(int))\n",
    "plotdf = plotdf.sort_values([\"metric\", \"prompt_key\"], ignore_index=True)\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=plotdf, x=\"prompt_key\", y=\"value\", col=\"metric\", col_wrap=3, kind=\"bar\", sharey=False, aspect=1.602\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\", font_scale=1.15)\n",
    "\n",
    "plotdf = (\n",
    "    df_agg_wide.loc[df_agg_wide[\"zerodiv_opt\"] == \"nan\"]\n",
    "    .loc[df_agg_wide[\"metric\"] == \"recall\"]\n",
    "    # .loc[df_agg_wide[\"prompt_key\"].astype(int).isin(np.arange(1, 11))]\n",
    "    # .loc[df_agg_wide[\"prompt_key\"].astype(int).isin(np.arange(11, 21))]\n",
    "    # .loc[df_agg_wide[\"prompt_key\"].astype(int).isin(np.arange(21, 31))]\n",
    "    .drop(columns=[\"zerodiv_opt\"])\n",
    "    .melt(id_vars=[\"metric\", \"prompt_key\"], var_name=\"class\")\n",
    ")\n",
    "plotdf[\"prompt_key\"] = pd.Categorical(plotdf[\"prompt_key\"].astype(int))\n",
    "plotdf = plotdf.sort_values([\"prompt_key\", \"class\", \"metric\"], ignore_index=True)\n",
    "g = sns.catplot(data=plotdf, x=\"prompt_key\", y=\"value\", col=\"class\", col_wrap=3, kind=\"bar\", sharey=True, aspect=1.602)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrices for each class\n",
    "mcm = multilabel_confusion_matrix(ytrue, ypred)\n",
    "\n",
    "\n",
    "# Function to plot confusion matrices for each class\n",
    "def plot_multilabel_confusion_matrix(mcm, classes):\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=int(len(classes) / 2) + len(classes) % 2, ncols=2, figsize=(10, 4 * int(len(classes) / 2))\n",
    "    )\n",
    "    for i, (cm, class_name) in enumerate(zip(mcm, classes)):\n",
    "        ax = axes.flatten()[i]\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", ax=ax, cmap=\"Blues\")\n",
    "        ax.set_xlabel(\"Predicted labels\")\n",
    "        ax.set_ylabel(\"True labels\")\n",
    "        ax.set_title(f\"Class: {class_name}\")\n",
    "        ax.xaxis.set_ticklabels([\"Negative\", \"Positive\"])\n",
    "        ax.yaxis.set_ticklabels([\"Negative\", \"Positive\"])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plotting the confusion matrices\n",
    "plot_multilabel_confusion_matrix(mcm, mlb.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrices = {label: {\"TP\": 0, \"FP\": 0, \"TN\": 0, \"FN\": 0} for label in classes}\n",
    "\n",
    "\n",
    "def update_conf_matrix(row, label):\n",
    "    if label in row[\"true_positives\"]:\n",
    "        conf_matrices[label][\"TP\"] += 1\n",
    "    if label in row[\"false_positives\"]:\n",
    "        conf_matrices[label][\"FP\"] += 1\n",
    "    if label in row[\"true_negatives\"]:\n",
    "        conf_matrices[label][\"TN\"] += 1\n",
    "    if label in row[\"false_negatives\"]:\n",
    "        conf_matrices[label][\"FN\"] += 1\n",
    "\n",
    "\n",
    "for label in classes:\n",
    "    results.apply(lambda row: update_conf_matrix(row, label), axis=1)\n",
    "\n",
    "\n",
    "conf_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "for label, matrix in conf_matrices.items():\n",
    "    cm = np.array([[matrix[\"TP\"], matrix[\"FP\"]], [matrix[\"FN\"], matrix[\"TN\"]]])\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix for {label}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histogram on `response_num_findings`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\", font_scale=1.15)\n",
    "\n",
    "g = sns.displot(\n",
    "    data=results,\n",
    "    x=\"response_num_findings\",\n",
    "    hue=\"label_in_response\",\n",
    "    multiple=\"stack\",\n",
    "    discrete=True,\n",
    "    aspect=GOLDEN,\n",
    "    height=6,\n",
    ")\n",
    "\n",
    "g.set_axis_labels(x_var=\"No. of findings in response\")\n",
    "sns.move_legend(g, \"center right\", bbox_to_anchor=(1, 0.5), title=\"Label in response\")\n",
    "g.figure.suptitle(\"Number of findings within generated response\")\n",
    "g.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\", font_scale=1.15)\n",
    "\n",
    "g = sns.displot(\n",
    "    data=results,\n",
    "    x=\"response_num_findings\",\n",
    "    hue=\"prompt_key\",\n",
    "    multiple=\"stack\",\n",
    "    discrete=True,\n",
    "    aspect=GOLDEN,\n",
    "    height=6,\n",
    ")\n",
    "\n",
    "g.set_axis_labels(x_var=\"No. of findings in response\")\n",
    "sns.move_legend(g, \"center right\", bbox_to_anchor=(1, 0.5), title=\"Prompt Key\")\n",
    "g.figure.suptitle(\"Number of findings within generated response\")\n",
    "g.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"notebook\", style=\"dark\", font_scale=1.15)\n",
    "\n",
    "g = sns.displot(\n",
    "    data=results,\n",
    "    x=\"response_num_findings\",\n",
    "    hue=\"prompt_key\",\n",
    "    multiple=\"fill\",\n",
    "    discrete=True,\n",
    "    aspect=GOLDEN,\n",
    "    height=6,\n",
    ")\n",
    "\n",
    "g.set_axis_labels(x_var=\"No. of findings in response\", y_var=\"Proportion\")\n",
    "sns.move_legend(g, \"center right\", bbox_to_anchor=(1, 0.5), title=\"Prompt Key\")\n",
    "g.figure.suptitle(\"Share of no. of findings within generated response by prompt\")\n",
    "g.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\", font_scale=1.15)\n",
    "\n",
    "g = sns.displot(\n",
    "    data=results,\n",
    "    x=\"response_num_findings\",\n",
    "    hue=\"finding_labels\",\n",
    "    multiple=\"stack\",\n",
    "    discrete=True,\n",
    "    aspect=GOLDEN,\n",
    "    height=6,\n",
    "    palette=\"tab20\",\n",
    ")\n",
    "\n",
    "g.set_axis_labels(x_var=\"No. of findings in response\")\n",
    "sns.move_legend(g, \"center right\", bbox_to_anchor=(1, 0.5), title=\"GT label\")\n",
    "g.figure.suptitle(\"Number of findings within generated response\")\n",
    "g.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\", font_scale=1.15)\n",
    "\n",
    "g = sns.displot(\n",
    "    data=results,\n",
    "    x=\"response_num_findings\",\n",
    "    hue=\"finding_labels\",\n",
    "    multiple=\"fill\",\n",
    "    discrete=True,\n",
    "    aspect=GOLDEN,\n",
    "    height=6,\n",
    "    palette=\"tab20\",\n",
    ")\n",
    "\n",
    "g.set_axis_labels(x_var=\"No. of findings in response\", y_var=\"Proportion\")\n",
    "sns.move_legend(g, \"center right\", bbox_to_anchor=(1, 0.5), title=\"GT label\")\n",
    "g.figure.suptitle(\"Share of no. of findings within generated response by GT label\")\n",
    "g.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\", font_scale=1.15)\n",
    "\n",
    "g = sns.displot(\n",
    "    data=results,\n",
    "    x=\"response_num_findings\",\n",
    "    col=\"prompt_key\",\n",
    "    col_wrap=5,\n",
    "    height=3.5,\n",
    "    hue=\"label_in_response\",\n",
    "    multiple=\"stack\",\n",
    "    discrete=True,\n",
    "    # palette=\"tab20\",\n",
    ")\n",
    "\n",
    "g.set_axis_labels(x_var=\"No. of findings in response\")\n",
    "sns.move_legend(g, \"right\", title=\"Label in response\")\n",
    "g.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\", font_scale=1.15)\n",
    "\n",
    "g = sns.displot(\n",
    "    data=results,\n",
    "    x=\"response_num_findings\",\n",
    "    col=\"prompt_key\",\n",
    "    col_wrap=5,\n",
    "    height=3.5,\n",
    "    hue=\"finding_labels\",\n",
    "    multiple=\"stack\",\n",
    "    discrete=True,\n",
    "    palette=\"tab20\",\n",
    ")\n",
    "\n",
    "g.set_axis_labels(x_var=\"Resp. number of findings\")\n",
    "g.legend.set_title(\"Finding\")\n",
    "g.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histogram on `prompt_key`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\", font_scale=1.15)\n",
    "\n",
    "g = sns.displot(\n",
    "    data=results,\n",
    "    x=\"prompt_key\",\n",
    "    hue=\"label_in_response\",\n",
    "    multiple=\"stack\",\n",
    "    discrete=True,\n",
    "    aspect=GOLDEN,\n",
    "    height=6,\n",
    ")\n",
    "\n",
    "g.set_axis_labels(x_var=\"Prompt Key\")\n",
    "sns.move_legend(g, \"center right\", bbox_to_anchor=(1, 0.5), title=\"Label in response\")\n",
    "g.figure.suptitle(\"Is label in generated response?\")\n",
    "g.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"notebook\", style=\"dark\", font_scale=1.15)\n",
    "\n",
    "g = sns.displot(\n",
    "    data=results,\n",
    "    x=\"prompt_key\",\n",
    "    hue=\"label_in_response\",\n",
    "    multiple=\"fill\",\n",
    "    discrete=True,\n",
    "    aspect=GOLDEN,\n",
    "    height=6,\n",
    ")\n",
    "\n",
    "g.set_axis_labels(x_var=\"Prompt Key\", y_var=\"Proportion\")\n",
    "sns.move_legend(g, \"center right\", bbox_to_anchor=(1, 0.5), title=\"Label in response\")\n",
    "g.figure.suptitle(\"Share of generated responses with correct label\")\n",
    "g.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"notebook\", style=\"dark\", font_scale=1.15)\n",
    "\n",
    "g = sns.displot(\n",
    "    data=results,\n",
    "    x=\"prompt_key\",\n",
    "    hue=\"response_num_findings\",\n",
    "    multiple=\"fill\",\n",
    "    discrete=True,\n",
    "    aspect=GOLDEN,\n",
    "    height=6,\n",
    "    palette=\"tab20\",\n",
    ")\n",
    "\n",
    "g.set_axis_labels(x_var=\"Prompt Key\", y_var=\"Proportion\")\n",
    "sns.move_legend(g, \"center right\", bbox_to_anchor=(0.85, 0.5), title=\"No. labels\\nin response\")\n",
    "g.figure.suptitle(\"Share of number of findings within generated response by prompt\")\n",
    "g.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Heatmaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "plotdf = (\n",
    "    results.groupby([\"prompt_key\", \"response_num_findings\"], observed=True)\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .pivot(columns=\"prompt_key\", index=\"response_num_findings\", values=0)\n",
    "    .iloc[::-1]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(GOLDEN * 6, 6))\n",
    "ax = sns.heatmap(plotdf, annot=True, fmt=\".0f\")\n",
    "ax.set(title=\"Number of findings in response by prompt\", xlabel=\"Prompt Key\", ylabel=\"Number of findings in response\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\", font_scale=1)\n",
    "\n",
    "plotdf = (\n",
    "    results.groupby([\"finding_labels\", \"response_num_findings\"], observed=True)\n",
    "    .size()\n",
    "    .reset_index()\n",
    "    .pivot(columns=\"finding_labels\", index=\"response_num_findings\", values=0)\n",
    "    .transform(lambda x: (x / x.sum()) * 100)\n",
    "    .iloc[::-1]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(GOLDEN * 8, 8))\n",
    "ax = sns.heatmap(plotdf, annot=True, fmt=\".1f\")\n",
    "ax.set(\n",
    "    title=\"Number of findings in response by disease\\n(values are percentages, columns sum to 100%)\",\n",
    "    xlabel=\"Disease\",\n",
    "    ylabel=\"Number of findings in response\",\n",
    ")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-finding imags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\", font_scale=1.15)\n",
    "\n",
    "g = sns.relplot(\n",
    "    data=results,\n",
    "    x=\"response_num_findings\",\n",
    "    hue=\"label_in_response\",\n",
    "    multiple=\"stack\",\n",
    "    discrete=True,\n",
    "    aspect=GOLDEN,\n",
    "    height=6,\n",
    ")\n",
    "\n",
    "g.set_axis_labels(x_var=\"No. of findings in response\")\n",
    "sns.move_legend(g, \"center right\", bbox_to_anchor=(1, 0.5), title=\"Label in response\")\n",
    "g.figure.suptitle(\"Number of findings within generated response\")\n",
    "g.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (chexagent)",
   "language": "python",
   "name": "chexagent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
