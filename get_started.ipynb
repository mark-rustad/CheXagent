{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from PIL import Image\n",
    "from rich import print\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url):\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return Image.open(io.BytesIO(resp.content)).convert(\"RGB\")\n",
    "\n",
    "\n",
    "def generate(images, prompt, processor, model, device, dtype, generation_config):\n",
    "    inputs = processor(images=images[:2], text=f\" USER: <s>{prompt} ASSISTANT: <s>\", return_tensors=\"pt\").to(\n",
    "        device=device, dtype=dtype\n",
    "    )\n",
    "    output = model.generate(**inputs, generation_config=generation_config)[0]\n",
    "    response = processor.tokenizer.decode(output, skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "\n",
    "def main():\n",
    "    # step 1: Setup constant\n",
    "    device = \"cuda\"\n",
    "    dtype = torch.float16\n",
    "\n",
    "    # step 2: Load Processor and Model\n",
    "    processor = AutoProcessor.from_pretrained(\"StanfordAIMI/CheXagent-8b\", trust_remote_code=True)\n",
    "    generation_config = GenerationConfig.from_pretrained(\"StanfordAIMI/CheXagent-8b\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"StanfordAIMI/CheXagent-8b\", torch_dtype=dtype, trust_remote_code=True\n",
    "    ).to(device)\n",
    "\n",
    "    # step 3: Fetch the images\n",
    "    image_path = \"https://upload.wikimedia.org/wikipedia/commons/3/3b/Pleural_effusion-Metastatic_breast_carcinoma_Case_166_%285477628658%29.jpg\"\n",
    "    images = [download_image(image_path)]\n",
    "\n",
    "    # step 4: Generate the Findings section\n",
    "    for anatomy in anatomies:\n",
    "        prompt = f'Describe \"{anatomy}\"'\n",
    "        response = generate(images, prompt, processor, model, device, dtype, generation_config)\n",
    "        print(f\"Generating the Findings for [{anatomy}]:\")\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `main()` call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anatomies = [\n",
    "    \"Airway\",\n",
    "    \"Breathing\",\n",
    "    \"Cardiac\",\n",
    "    \"Diaphragm\",\n",
    "    \"Everything else (e.g., mediastinal contours, bones, soft tissues, tubes, valves, and pacemakers)\",\n",
    "]\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [NIH Chest X-ray dataset](https://www.kaggle.com/datasets/nih-chest-xrays/sample/data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"image_index\", \"finding_labels\", \"follow_up_number\", \"patient_id\", \"patient_age\", \"patient_gender\", \"view_position\", \"original_image_width\", \"original_image_height\", \"original_image_pixel_spacing_x\", \"original_image_pixel_spacing_y\"]  # fmt: skip # nopep8\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"/data/irf/ai/tmp_MDR/github/MDR_CheXagent/data/NIH_Chest_X-ray_Dataset/Data_Entry_2017.csv\",\n",
    "    names=column_names,\n",
    "    header=0,\n",
    "    index_col=False,\n",
    ")\n",
    "print(f\"Rows: {data.shape[0]:,}\\tColumns: {data.shape[1]}\")\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=data[\"patient_gender\"])\n",
    "plt.gca().set(title=\"Distribution of Patient Gender\", xlabel=None, ylabel=\"Counts\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=data[data[\"patient_age\"] < 130], x=\"patient_age\", bins=20, kde=True)\n",
    "plt.gca().set(title=\"Distribution of Patient Age\", xlabel=\"Age\", ylabel=\"Counts\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boolean columns for each pathology type\n",
    "pathology_list = [\"Atelectasis\", \"Cardiomegaly\", \"Consolidation\", \"Edema\", \"Effusion\", \"Emphysema\", \"Fibrosis\", \"Hernia\", \"Infiltration\", \"Mass\", \"Nodule\", \"Pleural_Thickening\", \"Pneumonia\", \"Pneumothorax\"]  # fmt: skip # nopep8\n",
    "\n",
    "for pathology in pathology_list:\n",
    "    data[pathology] = data[\"finding_labels\"].apply(lambda x: 1 if pathology in x else 0)\n",
    "\n",
    "data[\"No_Findings\"] = data[\"finding_labels\"].apply(lambda x: 1 if \"No Finding\" in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of values across selected columns and reset index for Seaborn\n",
    "sum_data = data.iloc[:, 11:].sum().reset_index()\n",
    "sum_data.columns = [\"Feature\", \"Total\"]\n",
    "\n",
    "sns.barplot(x=\"Total\", y=\"Feature\", data=sum_data)\n",
    "plt.gca().set(title=\"Disease Classes\", xlabel=\"Counts\", ylabel=None)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"Total\", y=\"Feature\", data=sum_data.loc[sum_data[\"Feature\"] != \"No_Findings\", :])\n",
    "plt.gca().set(title=\"Disease Classes (exluding no disease class)\", xlabel=\"Counts\", ylabel=None)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset: cases with exactly one finding label, first case of each label\n",
    "subset = data.loc[data.iloc[:, 11:].apply(sum, axis=1) == 1, :].groupby(\"finding_labels\").head(1).copy()\n",
    "\n",
    "image_dir = \"/data/irf/ai/tmp_MDR/github/MDR_CheXagent/data/NIH_Chest_X-ray_Dataset\"\n",
    "subset[\"image_path\"] = subset[\"image_index\"].transform(lambda x: next(Path(image_dir).rglob(x)).as_posix())\n",
    "display(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `main()` code sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### step 1: Setup constant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dtype = torch.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### step 2: Load Processor and Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"StanfordAIMI/CheXagent-8b\", trust_remote_code=True)\n",
    "generation_config = GenerationConfig.from_pretrained(\"StanfordAIMI/CheXagent-8b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"StanfordAIMI/CheXagent-8b\", torch_dtype=dtype, trust_remote_code=True).to(\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### step 3: Fetch the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"https://upload.wikimedia.org/wikipedia/commons/3/3b/Pleural_effusion-Metastatic_breast_carcinoma_Case_166_%285477628658%29.jpg\"\n",
    "images = [download_image(image_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### step 4: Generate the Findings section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anatomies = [\n",
    "    \"Airway\",\n",
    "    \"Breathing\",\n",
    "    \"Cardiac\",\n",
    "    \"Diaphragm\",\n",
    "    \"Everything else (e.g., mediastinal contours, bones, soft tissues, tubes, valves, and pacemakers)\",\n",
    "]\n",
    "\n",
    "for anatomy in anatomies:\n",
    "    prompt = f'Describe \"{anatomy}\"'\n",
    "    response = generate(images, prompt, processor, model, device, dtype, generation_config)\n",
    "    print(f\"Generating the Findings for [{anatomy}]:\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Identify all abnormalities in the given chest X-ray.\",\n",
    "    \"What abnormalities are notable for this patient?\",\n",
    "    \"Write a structured Findings section for the given images as if you are a radiologist.\",\n",
    "    # \"Assess the chest X-ray, identify key findings in the CXR and write a structured findings section.\",\n",
    "    \"Are there any ground glass opacities?\",\n",
    "    # \"Put a bounding box around regions showing pleural effusion?\"\n",
    "]\n",
    "\n",
    "for idx, row in subset.iterrows():\n",
    "    image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    img_name, label = row[\"image_index\"], row[\"finding_labels\"]\n",
    "    plt.title(f\"{img_name} Label: {label}\")\n",
    "    plt.show()\n",
    "    for prompt in prompts:\n",
    "        inputs = processor(images=[image], text=f\" USER: <s>{prompt} ASSISTANT: <s>\", return_tensors=\"pt\").to(\n",
    "            device=device, dtype=dtype\n",
    "        )\n",
    "        output = model.generate(**inputs, generation_config=generation_config)[0]\n",
    "        response = processor.tokenizer.decode(output, skip_special_tokens=True)\n",
    "        formatted_response = \"\\n\".join(\n",
    "            textwrap.wrap(response, width=100, break_long_words=False, replace_whitespace=False)\n",
    "        )\n",
    "        print(f\"\\n\\nQ: {prompt}\\n\\nA: {formatted_response}\")\n",
    "    print(\"-\" * 100, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
